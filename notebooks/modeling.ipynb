{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Development & Tuning\n",
                "\n",
                "## Objectives\n",
                "1. Load and preprocess the dataset.\n",
                "2. Vectorize text using TF-IDF.\n",
                "3. Train and tune three models:\n",
                "    - Logistic Regression (Baseline)\n",
                "    - Linear SVM\n",
                "    - LightGBM\n",
                "4. Compare performance and select the best model.\n",
                "5. Save the best model and vectorizer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pickle\n",
                "import os\n",
                "\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.calibration import CalibratedClassifierCV\n",
                "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
                "import lightgbm as lgb\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set_style('whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data Shape: (49977, 7)\n",
                        "label\n",
                        "0    48678\n",
                        "1     1299\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "DATA_PATH = '../data/raw'\n",
                "\n",
                "def load_data():\n",
                "    try:\n",
                "        auth = pd.read_csv(os.path.join(DATA_PATH, 'Authentic-48K.csv'))\n",
                "        fake = pd.read_csv(os.path.join(DATA_PATH, 'Fake-1K.csv'))\n",
                "        \n",
                "        auth['label'] = 0\n",
                "        fake['label'] = 1\n",
                "        \n",
                "        # Use a smaller sample of authentic news to balance slightly if needed, \n",
                "        # but for this capstone we will use all or a reasonable ratio.\n",
                "        # Let's use all for now, but be aware of class imbalance.\n",
                "        \n",
                "        df = pd.concat([auth, fake], ignore_index=True)\n",
                "        # Shuffle\n",
                "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "        return df\n",
                "    except FileNotFoundError:\n",
                "        print(\"Data not found. Please ensure data is in data/raw\")\n",
                "        return None\n",
                "\n",
                "df = load_data()\n",
                "print(f\"Data Shape: {df.shape}\")\n",
                "print(df['label'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing & Vectorization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train Shape: (39981,)\n",
                        "Test Shape: (9996,)\n"
                    ]
                }
            ],
            "source": [
                "# Setup - Ensure content is string\n",
                "df['content'] = df['content'].fillna('').astype(str)\n",
                "\n",
                "# Split Data\n",
                "X = df['content']\n",
                "y = df['label']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(f\"Train Shape: {X_train.shape}\")\n",
                "print(f\"Test Shape: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TF-IDF Matrix Shape: (39981, 5000)\n"
                    ]
                }
            ],
            "source": [
                "# TF-IDF Vectorization\n",
                "# Using max_features to keep dimensions manageable and reduce noise\n",
                "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
                "\n",
                "X_train_tfidf = tfidf.fit_transform(X_train)\n",
                "X_test_tfidf = tfidf.transform(X_test)\n",
                "\n",
                "print(f\"TF-IDF Matrix Shape: {X_train_tfidf.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Training & Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper function for evaluation\n",
                "results = []\n",
                "\n",
                "def evaluate_model(name, model, X_test, y_test):\n",
                "    y_pred = model.predict(X_test)\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    print(f\"--- {name} ---\")\n",
                "    print(f\"Accuracy: {acc:.4f}\")\n",
                "    print(f\"F1 Score: {f1:.4f}\")\n",
                "    print(classification_report(y_test, y_pred))\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Accuracy': acc,\n",
                "        'F1 Score': f1\n",
                "    })"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model 1: Logistic Regression (Baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr_params = {'C': [0.1, 1, 10]}\n",
                "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
                "\n",
                "grid_lr = GridSearchCV(lr, lr_params, cv=3, scoring='f1', n_jobs=-1)\n",
                "grid_lr.fit(X_train_tfidf, y_train)\n",
                "\n",
                "print(f\"Best LR Params: {grid_lr.best_params_}\")\n",
                "evaluate_model('Logistic Regression', grid_lr.best_estimator_, X_test_tfidf, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model 2: Linear SVM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "svm_params = {'C': [0.1, 1, 10]}\n",
                "svm = LinearSVC(random_state=42, dual='auto') # dual='auto' to suppress warnings for n_samples > n_features\n",
                "\n",
                "# LinearSVC does not support predict_proba by default, which we need for Risk Score.\n",
                "# We can use CalibratedClassifierCV if we select this as best, or just use the decision function.\n",
                "# For now, simple GridSearch.\n",
                "\n",
                "grid_svm = GridSearchCV(svm, svm_params, cv=3, scoring='f1', n_jobs=-1)\n",
                "grid_svm.fit(X_train_tfidf, y_train)\n",
                "\n",
                "print(f\"Best SVM Params: {grid_svm.best_params_}\")\n",
                "evaluate_model('Linear SVM', grid_svm.best_estimator_, X_test_tfidf, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model 3: LightGBM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lgb_params = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'learning_rate': [0.05, 0.1],\n",
                "    'num_leaves': [31, 50],\n",
                "    'device': ['gpu'] # Attempt to use GPU\n",
                "}\n",
                "\n",
                "print(\"Training LightGBM... (Attempting GPU usage)\")\n",
                "try:\n",
                "    lgbm = lgb.LGBMClassifier(random_state=42, verbose=-1, force_col_wise=True)\n",
                "    grid_lgbm = GridSearchCV(lgbm, lgb_params, cv=3, scoring='f1', n_jobs=-1)\n",
                "    grid_lgbm.fit(X_train_tfidf, y_train)\n",
                "except Exception as e:\n",
                "    print(f\"GPU Training failed or skipped: {e}. Falling back to CPU.\")\n",
                "    # Fallback to CPU by removing device param or setting to cpu\n",
                "    lgb_params['device'] = ['cpu']\n",
                "    lgbm = lgb.LGBMClassifier(random_state=42, verbose=-1, force_col_wise=True)\n",
                "    grid_lgbm = GridSearchCV(lgbm, lgb_params, cv=3, scoring='f1', n_jobs=-1)\n",
                "    grid_lgbm.fit(X_train_tfidf, y_train)\n",
                "\n",
                "print(f\"Best LightGBM Params: {grid_lgbm.best_params_}\")\n",
                "evaluate_model('LightGBM', grid_lgbm.best_estimator_, X_test_tfidf, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(results)\n",
                "print(results_df)\n",
                "\n",
                "# Identify best model based on F1 Score\n",
                "best_model_name = results_df.sort_values(by='F1 Score', ascending=False).iloc[0]['Model']\n",
                "print(f\"\\nBest Model: {best_model_name}\")\n",
                "\n",
                "if best_model_name == 'Logistic Regression':\n",
                "    best_model = grid_lr.best_estimator_\n",
                "elif best_model_name == 'Linear SVM':\n",
                "    # Calibrate SVM for probability output\n",
                "    best_model = CalibratedClassifierCV(grid_svm.best_estimator_, method='sigmoid', cv='prefit')\n",
                "    best_model.fit(X_test_tfidf, y_test) # Note: this is a bit cheaty using test for calib, strictly should use val.\n",
                "    # For simplicity in this notebook, we might fallback to LR if SVM is close, or stick to proper split.\n",
                "    # Let's simple re-fit Calibrated on Train for correctness if chosen.\n",
                "    best_model = CalibratedClassifierCV(grid_svm.best_estimator_, method='sigmoid', cv=3)\n",
                "    best_model.fit(X_train_tfidf, y_train)\n",
                "else:\n",
                "    best_model = grid_lgbm.best_estimator_"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Artifacts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists('../models'):\n",
                "    os.makedirs('../models')\n",
                "\n",
                "# Save Vectorizer\n",
                "with open('../models/tfidf_vectorizer.pkl', 'wb') as f:\n",
                "    pickle.dump(tfidf, f)\n",
                "\n",
                "# Save Model\n",
                "with open('../models/best_model.pkl', 'wb') as f:\n",
                "    pickle.dump(best_model, f)\n",
                "\n",
                "print(\"Artifacts saved to ../models/\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ml",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
